---
title: 'Optimal Driver Node Selection (Part 3): Proving the Ellipsoid Maximization Problem is Submodular'
date: 2021-01-18
permalink: /posts/2021/01/logdet/
tags:
  - optimization
  - driver nodes
  - set functions
  - heuristics
---

## OUTLINE

This post combines the results of the previous two posts in this series: LINK HERE.
I will follow CITATION HERE in their proof with some further elaboration.
The rest of the post explores the usefulness of this result which is not all positive.

1. Proof that $$ h(\mathcal{D}) = \det W_{\mathcal{D}} $$ is NOT submodular while $$ h(\mathcal{D}) = \log \det W_{\mathcal{D}} $$ is submodular.
2. Exploring the greedy heuristic's approximation guarantee using minimal controllability sets.
3. Numerical studies for large networks.

## Review of the Ellipsoid Maximization Problem

In the first post, we showed that the optimal driver node placement problem is equivalent to the ellipsoid maximization problem,

\begin{equation}
  \begin{array}{cl}
    \min\limits_{\mathcal{D} \subseteq \mathcal{V}} & \det W_{\mathcal{D}}\newline
    \text{s.t.} & |\mathcal{D}| = m
  \end{array}
\end{equation}

To use the result of the approximation guarantee for the greedy heuristic applied to submodular set function maximization, we would need to show $$ h(\mathcal{D}) = \det W_{\mathcal{D}} $$ is a submodular set function.
Unfortunately, it is not a submodular set function which can be shown by example.

Luckily though, its logarithm is submodular which is shown in the next series of theorems.

{%
  include lemma.html
  title="Jacobi's Formula"
  text="Let $$ W : \mathbb{R} \mapsto \mathbb{R}^{n \times n} $$ be a non-singular matrix function , then the derivative of its determinant is 
  
  \begin{equation}
    \frac{d}{dz} \det W(z) = \det W(z) \text{Tr} \left( W^{-1}(z) \frac{d}{dz} W(z) \right)
  \end{equation}"
  proof="The proof of Jacobi's formula can be found in many places, such as Wikipedia, and requires techniques outside of the scope of this post."
%}

<br/>

{%
  include lemma.html
  title="Similar Matrices"
  text="Let \( W \) be a square matrix and let \( S \) be a non-singular matrix.
  Then if \( \mu \) is an eigenvalue of \( W \) with eigenvector \( \textbf{v} \), then \( \mu \) is also an eigenvalue of \( SWS^{-1} \) with eigenvector \( S \textbf{v} \)."
  proof="Begin with the definition of an eigenvalue/eigenvector pair for a matrix,
  
  \begin{equation*}
      W \textbf{v} = W S^{-1} S \textbf{v} = \mu \textbf{v}
  \end{equation*}
  
  Pre-multiply by \( S \) 
  
  \begin{equation*}
    S W S^{-1} S \textbf{v} = \mu S \textbf{v}
  \end{equation*}
  
  From this expression, the matrix \( SWS^{-1} \) has an eigenvalue \( \mu \) with eigenvector \( S \textbf{v} \)."
%}

<br/>

{%
  include lemma.html
  title="PSD Preserving Transformation"
  text="Let \( W \) be a PSD matrix, then for any square matrix \( S \) the matrix \( S W S^T \) is also PSD."
  proof="Simply use the definition of a PSD matrix,
  
  \begin{equation*}
    \textbf{x}^T S W S^T \textbf{x} = \textbf{z}^T W \textbf{z} \geq 0, \quad \forall \textbf{x} \in \mathbb{R}^n
  \end{equation*}
  
  where \( \textbf{z} = S^T \textbf{x} \)."
%}
<br/>

{%
  include lemma.html
  title="Ordering of PD Matrices"
  text="Let \( W_1 \) and \( W_2 \) be two symmetric PD matrices such that \( W_2 - W_1 \) is a PSD matrix. Then \( W_1^{-1} - W_2^{-1} \) is also a PSD matrix."
  proof="
  Before addressing the statement in the lemma, a related statement is proved.
  Let \( W \) be a symmetric PD matrix, then if \( I - W \) is PSD then \( W^{-1} - I \) is PSD.
  To prove this statement, use the square root factorization of a PD matrix, \( W = W^{1/2} W^{1/2} \), and multiply both sides of the expression of interest by \( W^{-1/2} \).
  
  \begin{equation}
    W^{-1/2} (I-W) W^{-1/2} = (W^{-1} - I) 
    \label{eq:IW}
  \end{equation}
  
  Using the Lemma CITE PSD PRESERVING, if \( I-W \) is PSD, then so is \( W^{-1} - I \).
  
  Turning to the main statement of the lemma, multiply both sides by \( W_2^{-1/2} \),
  
  \begin{equation*}
    W_2^{-1/2} (W_2 - W_1) W^{-1/2} = (I - W_2^{-1/2} W_1 W_2^{-1/2})
  \end{equation*}
  
  Using Eq. \eqref{eq:IW}, 
  
  \begin{equation*}
    (I - W_2^{-1/2} W_1 W_2^{-1/2}) \text{ is PSD } \Rightarrow (W_2^{1/2} W_1^{-1} W_2^{1/2} - I) \text{ is PSD }
  \end{equation*}
  
  Multiply the final statement on both sides by \( W_2^{-1/2} \),
  
  \begin{equation*}
    W_2^{-1/2} (W_2^{1/2} W_1^{-1} W_2^{1/2} - I) W_2^{-1/2} = (W_1^{-1} - W_2^{-1} )
  \end{equation*}
  "
%}

<br/>

{%
  include lemma.html
  title="Product of PD and PSD Matrices"
  text="Let \( W_1 \) and \( W_2 \) be a symmetric PSD matrices.
  Then the product \( W_1 W_2 \) is a PSD matrix."
  proof="
  The full proof can be found in Bernstein CITE BERNSTEIN which requires some technical steps beyond the scope of this post.
  A simpler case to give the flavor of the proof is the case \( W_1 \) is PD and \( W_2 \) is PSD, and it is presented here.
  
  As \( W_1 \) is symmetric and PD, its eigendecomposition can be written in terms of an orthogonal matrix \( V_1 \) and strictly positive diagonal matrix \( \mathcal{M}_1 \),
  
  \begin{equation*}
    W_1 V_1 = V_1 \mathcal{M}_1
  \end{equation*}
  
  Define the matrix \( S_1 = \mathcal{M}_1^{-1/2} V_1^T \) so that \( S_1 W_1 S_1^T = I_n \).
  Define the transformed matrix \( \bar{W}_2 = S_1^{-T} W_2 S_1^{-1} \).
  By Lemma CITE HERE since \( W_2 \) is PSD, so is \( \bar{W}_2 \).
  The eigendecomposition of \( \bar{W}_2 \) consists of the orthogonal matrix \( \bar{V}_2 \) and the non-negative diagonal matrix \( \bar{\mathcal{M}}_2 \).
  
  \begin{equation*}
    \bar{W}_2 \bar{V}_2 = \bar{V}_2 \bar{\mathcal{M}}_2
  \end{equation*}
  
  so that \( \bar{V}_2^T \bar{W}_2 \bar{V}_2 = \bar{\mathcal{M}}_2 \).
  Define the matrix \( S = \bar{V}_2^T \mathcal{M}_1^{-1/2} V_1^T \) which is non-singular as each of its components is non-singular.
  Now compute the product,
  
  \begin{equation*}
    \begin{aligned}
      SW_1 W_2 S^{-1} &= S W_1 S^T S^{-T} W_2 S^{-1}\\
      &= \bar{V}_2^T I_n \bar{V}_2 \bar{V}_2^T \bar{W}_2 \bar{V}_2\\
      &= I_n \mathcal{M}_2\\
      &= \mathcal{M}_2
    \end{aligned}
  \end{equation*}
  
  This result proves that \( S W_1 W_2 S^{-1} \) is PSD,
  
  \begin{equation*}
    \textbf{x}^T S W_1 W_2 S^{-1} \textbf{x} = \textbf{x}^T \bar{\mathcal{M}}_2 \textbf{x} = \sum_{k=1}^n \bar{\mu}_2 x_k^2 \geq 0, \quad \forall \textbf{x} \in \mathbb{R}^n
  \end{equation*}
  
  By Lemma CITE HERE, as \( S W_1 W_2 S^{-1} \) is PSD, so is \( W_1 W_2 \) which completes the proof.
  "
%}

<br/>

{%
  include definition.html
  title="Trace of a Matrix"
  text="The trace of a square matrix \( W \) can equivalently be defined in two ways, as either the sum of the diagonal elements or the sum of its eigenvalues.
  
  \begin{equation*}
    \text{Tr}(W) = \sum_{k=1}^n W_{k,k} = \sum_{k=1}^n \mu_k
  \end{equation*}
  
  Some facts about the matrix trace are immediate:
  
  \begin{equation*}
    \text{Tr} (W_1 + W_2) = \text{Tr} (W_1) + \text{Tr} (W_2)
  \end{equation*}
  
  \begin{equation*}
    \text{Tr} (-W) = - \text{Tr} (W)
  \end{equation*}
  
  If the matrix \( W \) is PSD, then,
  
  \begin{equation*}
    \text{Tr} (W) \geq 0
  \end{equation*}
  "
%}

<br/>

As a reminder, the set of controllable subsets is defined as,

\begin{equation}
  \mathcal{C}(A) = \left\\{ \mathcal{D} \subseteq \mathcal{V} \vert \det W_{\mathcal{D}} > 0 \right\\}
\end{equation}

In the following, we are only interested in $$ \mathcal{D} \in \mathcal{C}(A) $$ as the logarithm of the determinant of the controllability Gramian is only defined when $$ W_{\mathcal{D}} $$ is positive definite.

<br/>

{%
  include theorem.html
  text="LogDet is a Submodular Set Function"
  text="The set function \( h (\mathcal{D}) = \log \det W_{\mathcal{D}} \) for \( \mathcal{D} \in \mathcal{C}(A) \) is submodular."
  proof="The proof uses the second definition of submodularity.
  
  \begin{equation*}
    h_j (\mathcal{D}_1) \geq h_j (\mathcal{D}_2), \quad \forall \mathcal{D}_1 \subseteq \mathcal{D}_2 \subset \mathcal{C}(A), \quad \forall j \in \mathcal{V} \backslash \mathcal{D}_2
  \end{equation*}
  
  where \( h_j \) is the incremental set function,
  
  \begin{equation*}
    h_j(\mathcal{D}) = h(\mathcal{D} \cup \{j\}) - h(\mathcal{D}) = \log \det (W_{\mathcal{D}} + W_{\{j\}}) - \log \det (W_{\mathcal{D}}), \quad \forall j \in \mathcal{V} \backslash \mathcal{D}
  \end{equation*}
  
  where I used the additivity property of the controllability Gramian.
  
  Define the matrix function 
  
  \begin{equation*}
    \Omega(z) = W_{\mathcal{D}_1} + z (W_{\mathcal{D}_2} - W_{\mathcal{D}_1}), \quad z \in [0,1]
  \end{equation*}
  
  so that \( \Omega (0) = W_{\mathcal{D}_1} \) and \( \Omega(1) = W_{\mathcal{D}_2} \).
  
  Define the smooth interpolating function between \( h_j(\mathcal{D}_1) \) and \( h_j(\mathcal{D}_2) \),
  
  \begin{equation*}
    \bar{h} = \log \det \left( \Omega (z) + W_{\{j\}} \right) - \log \det \left( \Omega (z) \right)
  \end{equation*}
  
  so \( \bar{h}(0) = h_j(\mathcal{D}_1) \) while \( \bar{h}(1) = h_j(\mathcal{D}_2) \).
  
  By the fundamental rule of calculus, 
  
  \begin{equation}
    \bar{h}(1) - \bar{h}(0) = h_j(\mathcal{D}_2) - h_j (\mathcal{D}_1) = \int_0^1 \frac{d \bar{h}(z)}{dz} dz
    \label{eq:froc}
  \end{equation}
  
  Evaluating the derivative of \( \bar{h} \),
  
  \begin{equation*}
    \begin{aligned}
      \frac{d\bar{h}(z)}{dz} &= \frac{d}{dz} \log \det (\Omega(z) + W_{\{j\}}) - \frac{d}{dz}\log \det (\Omega(z))\newline
      &= \frac{1}{\det (\Omega(z) + W_{\{j\}})} \frac{d}{dz} \det(\Omega (z) + W_{\{j\}}) - \frac{1}{\det \Omega(z)} \frac{d}{dz} \det \Omega(z)\newline
      &= \text{Tr}\left( (\Omega(z)+W_{\{j\}})^{-1} \frac{d}{dz} \Omega(z) \right) + \text{Tr}\left(\Omega^{-1}(z) \frac{d}{dz} \Omega(z) \right)\\
      &= -\text{Tr} \left(\left(\Omega^{-1}(z) - (\Omega(z) + W_{\{j\}})^{-1} \right) \left( W_{\mathcal{D}_2} - W_{\mathcal{D}_1} \right) \right)
    \end{aligned}
  \end{equation*}
  
  <ol>
    <li> The matrix \( \Omega(z) + W_{\{j\}} - \Omega(z) = W_{\{j\}} \) is PSD so by Lemma CITE INVERSION \( \Omega^{-1} (z) - (\Omega(z) + W_{\{j\}})^{-1}} \) is also PSD. </li>
    <li> The matrix \( W_{\mathcal{D}_2} - W_{\mathcal{D}_1} = W_{\mathcal{D}_2 \backslash \mathcal{D}_1} \) is PSD. </li>
    <li> By Lemma CITE PRODUCT, the product of PSD matrices is PSD. </li>
    <li> From Def CITE TRACE DEF, he trace of a PSD matrix is non-negative. </li>
  </ol>
  
  With this sequence of steps, we have shown
  
  \begin{equation*}
    - \text{Tr} \left( \left(\Omega^{-1} (z) - (\Omega(z) + W_{\{j\}})^{-1} \right) \left( W_{\mathcal{D}_2} - W_{\mathcal{D}_1} \right) \right) \leq 0
  \end{equation*}
  
  Plugging this result into Eq. \eqref{eq:froc} we get the inequality,
  
  \begin{equation}
    \bar{h}(1) - \bar{h}(0) = h_j(\mathcal{D}_2) - h_j(\mathcal{D}_1) = \int_0^1 \frac{d \bar{h}}{dz} dz \leq 0 \quad \rightarrow \quad h_j (\mathcal{D}_2) \leq h_j (\mathcal{D}_1)
  \end{equation}
  
  which is the definition of a submodular set function."
%}

<br/>

Returning to the modified ellipsoid maximization problem where we have now shown the cost function is an increasing submodular set function.

\begin{equation}
  \begin{array}{cl}
    \min\limits_{\mathcal{D} \in \mathcal{C}(A)} & \log \det W_{\mathcal{D}}\newline
    \text{s.t.} & |\mathcal{D}| = m
  \end{array}
  \label{eq:optlogdet}
\end{equation}

Applying the result of THEOREM LOGDET IS SUBMODULAR must be done carefully though.
Pick a set $$ \mathcal{D}_C \in \mathcal{C}(A) $$ where $$ \vert \mathcal{D}_C \vert = m_0 $$ to seed the greedy algorithm and denote $$ \mathcal{D}_C^* $$ to be a set that satisfies

\begin{equation}
  \mathcal{D}\_C^\* = \underset{\mathcal{D} \supset \mathcal{D}\_C, \vert \mathcal{D} \vert = m}{\text{arg}\max} \log\det W\_{\mathcal{D}}
\end{equation}

Then from THEOREM APPROXIMATION GUARANTEE OF THE GREEDY ALGORITHM, using the greedy heuristic will return a solution, $$ \mathcal{D}_C^G $$ that satisfies the bound,

\begin{equation}
  \log \det W\_{\mathcal{D}\_C^\*} - \left( \frac{m-m\_0-1}{m-m\_0} \right)^{m-m\_0} \left( \log \det W\_{\mathcal{D}\_C^*} - \log \det W\_{\mathcal{D}\_C} \right) \leq \log\det W\_{\mathcal{D}\_C^G} \leq \log \det W_{\mathcal{D}\_C^\*}
\end{equation}

Some important notes on this bound:

<ol>
  <li> The bound is not necessarily a function of the true optimal solution of the original optimization problem in Eq. \eqref{eq:optlogdet}. </li>
  <li> It is also not clear how to choose \( \mathcal{D}_C \) so that \( \mathcal{D}_C^G \), or even \( \mathcal{D}_C^* \), are good solutions with regard to the original optimization problem. </li>
</ol>
